# -*- coding: utf-8 -*-
"""Cópia de Projeto Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VChvD_U2alYwUICMbPgCq9nAonZxDyaL

Instalação pré-script
"""

# Instala pacotes com versões atualizadas e evita reinstalações desnecessárias
#pip install dash==2.12.1 jupyter-dash==0.4.2 dash-bootstrap-components plotly --upgrade --quiet

# Baixa o modelo de linguagem do spaCy em português
#python -m spacy download pt_core_news_sm

#!pip uninstall -y dash jupyter-dash dash-bootstrap-components
#!pip install dash==2.12.1 jupyter-dash==0.4.2 dash-bootstrap-components==1.4.1

# Manipulação de dados
import pandas as pd                     # Leitura e manipulação de dados tabulares (CSV, Excel, etc.)
import numpy as np                      # Operações matemáticas e vetoriais com arrays

# Visualização
import matplotlib.pyplot as plt         # Gráficos estáticos (linha, barras, dispersão)
import seaborn as sns                   # Gráficos estatísticos avançados (heatmaps, boxplots)
import plotly.express as px             # Gráficos interativos de alto nível
import plotly.graph_objects as go       # Gráficos interativos mais customizáveis com Plotly

# Machine Learning - Scikit-learn
from sklearn.pipeline import Pipeline   # Pipeline para juntar etapas de pré-processamento e modelagem
from sklearn.model_selection import train_test_split  # Separação de dados em treino e teste
from sklearn.impute import SimpleImputer             # Tratamento de valores ausentes
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Escalonamento e codificação
from sklearn.compose import ColumnTransformer         # Aplicar transformações por tipo de coluna

from sklearn.cluster import KMeans                   # Algoritmo de clusterização
from sklearn.decomposition import PCA                # Redução de dimensionalidade

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor  # Classificação e Regressão

from sklearn.feature_extraction.text import TfidfVectorizer  # Vetorização de texto para ML

from sklearn.metrics import (                        # Avaliação de modelos
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, ConfusionMatrixDisplay,
    silhouette_score, davies_bouldin_score
)

# Processamento de linguagem natural (PLN)
import nltk                                           # Biblioteca principal para PLN
from nltk.corpus import stopwords                    # Stopwords em português
from nltk.stem import RSLPStemmer                    # Stemmer para reduzir palavras ao radical
import spacy                                          # Análise linguística avançada (SpaCy)
import re                                             # Expressões regulares para limpeza de texto

# Análise de sentimentos em português
from pysentimiento import create_analyzer            # Classificador de sentimentos em português

# Big Data - PySpark
from pyspark.sql import SparkSession                 # Sessão Spark para processamento distribuído
from pyspark.ml.feature import VectorAssembler       # Combina colunas em vetor de features
from pyspark.ml.clustering import KMeans as SparkKMeans  # Versão Spark do KMeans

# Dashboard interativo com Dash
import dash                                           # Framework web para dashboards interativos
from dash import dcc, html, Input, Output, Dash           # Componentes essenciais do Dash
import dash_bootstrap_components as dbc              # Componentes prontos com estilos Bootstrap
import requests                                       # Requisições HTTP (ex: pegar GeoJSON de mapas)
from jupyter_dash import JupyterDash

# Outras utilidades
import sqlite3                                        # Banco de dados SQLite em memória
import os                                             # Acesso a arquivos e diretórios
import json                                           # Manipulação de arquivos e strings JSON

from google.colab import drive
drive.mount('/content/drive')

# === Leitura dos Dados ===

# =====================
# Carregamento de Dados
# =====================
df = pd.read_csv('/content/drive/MyDrive/Projeto/dadosTratados.csv', sep = ";")

"""***CLASSIFICAÇÃO***"""

#base de dados
data = pd.DataFrame({
    'texto':[
    "Me sinto muito bem com meu corpo de mulher.",
    "Nunca senti atração sexual por ninguém, e tá tudo bem.",
    "Cada pessoa tem sua própria vivência, né?",
    "Gosto do sexo oposto, sou hétero mesmo.",
    "Sou um homem trans e me orgulho da minha jornada.",
    "Me apaixono por homens e mulheres.",
    "Não me identifico com nenhum gênero fixo.",
    "Sou assexual e vivo bem assim.",
    "Sou um homem trans, ainda em transição.",
    "Sinto que ninguém me entende, me rotulam de inválido.",
    "Me maquiar me faz sentir mais feminina.",
    "Me identifico como bi, desde nova.",
    "Gênero pra mim é uma coisa muito fluida.",
    "Sigo meu coração, independentemente do gênero.",
    "Gosto de sair com quem me faz bem, sem rótulos.",
    "Sempre gostei de meninos e meninas.",
    "Me entendo como não-binário desde criança.",
    "Me apaixonei por alguém do mesmo sexo.",
    "Sou um homem cis, gosto do que sou.",
    "Sempre fui atraído por mulheres.",
    "As pessoas não me entendem, dizem que sou inválido.",
    "Não sinto desejo por ninguém, e tá tudo certo.",
    "Ser mulher é uma parte essencial de mim.",
    "Amo quem eu amo, sou gay com orgulho.",
    "Não me encaixo nas caixinhas que a sociedade impõe.",
    "Prefiro dizer que sou diferente, sem rótulos.",
    "Desde pequena me vejo como mulher.",
    "A ausência de desejo é minha realidade.",
    "O gênero é uma construção, me vejo fora dele.",
    "Me relaciono com qualquer pessoa, sou pan.",
    "Minha identidade como homem trans é firme.",
    "Falam que sou inválido só por ser diferente.",
    "Ser mulher é minha força.",
    "Me envolvo com pessoas, não gêneros.",
    "Me sinto confortável sendo só eu.",
    "Sempre gostei de quem tem o mesmo sexo.",
    "Sou um homem trans com muito orgulho.",
    "Gosto de mulheres, sou hétero.",
    "Finalmente me reconheço como mulher trans.",
    "Sigo meu coração, não o gênero da pessoa.",
    "Não tenho uma identidade de gênero fixa.",
    "Sempre me senti atraído por mulheres.",
    "Me reconheço como mulher trans, é libertador.",
    "Não curto rótulos, cada um vive sua verdade.",
    "Minha identidade é fora da norma.",
    "Me considero bi há muitos anos.",
    "Sou uma mulher trans feliz comigo mesma.",
    "Amo além do gênero, sou pansexual.",
    "Minha transição como homem está sendo linda.",
    "Amar alguém do mesmo sexo é natural.",
    "Me reconheço como homem desde sempre.",
    "Ninguém me entende, me chamam de inválido.",
],
    'rótulo': [1 if t not in ["Assexual", "Heterossexual", "Bissexual", "Homossexual", "Pansexual"] else 0 for t in [
    "Mulher Cisgenero", "Assexual", "Outros", "Heterossexual", "Homem Transgenero", "Bissexual",
    "Nao-Binario", "Assexual", "Homem Transgenerro", "Invalido", "Mulher Cisgenero", "Bissexual",
    "Nao-Binario", "Bissexual", "Outros", "Bissexual", "Nao-Binario", "Homossexual",
    "Homem Cisgenero", "Heterossexual", "Invalido", "Assexual", "Mulher Cisgenero", "Homossexual",
    "Nao-Binario", "Outros", "Mulher Cisgenero", "Assexual", "Nao-Binario", "Pansexual",
    "Homem Transgenero", "Invalido", "Mulher Cisgenero", "Bissexual", "Outros", "Homossexual",
    "Homem Transgenero", "Heterossexual", "Mulher Transgenero", "Pansexual", "Nao-Binario", "Heterossexual",
    "Mulher Transgenero", "Outros", "Nao-Binario", "Bissexual", "Mulher Transgenero", "Pansexual",
    "Homem Transgenero", "Homossexual", "Homem Cisgenero", "Invalido"
]]})

#removendo duplicatas
data = data.drop_duplicates()

#removendo textos irrelevantes
data = data[~data["texto"].isin(["Invalido", "Outros"])]
data = data[~data["rótulo"].isin(["Invalido", "Outros"])]

# PRÉ-PROCESSAMENTO DE TEXTO
nltk.download('stopwords')
nltk.download('rslp')
nlp = spacy.load('pt_core_news_sm')  # modelo spaCy em português
stop_words = set(stopwords.words('portuguese'))
stemmer = RSLPStemmer()

def preprocess_text(text):
    text = re.sub(r'[^a-zA-Záéíóúãõâêîôûç\s]', '', text)  # Remove tudo que não é letra
    text = text.lower()
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]
    tokens = [stemmer.stem(word) for word in tokens]
    return ' '.join(tokens)

# APLICANDO PRÉ-PROCESSAMENTO
data_com_preproc = data.copy()
data_com_preproc["texto"] = data_com_preproc["texto"].apply(preprocess_text)

# Separando dados
x = data['texto']
y = data['rótulo']

# TREINO/TESTE
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# PIPELINE DE CLASSIFICAÇÃO COM TFIDF + RANDOMFOREST
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=1)),
    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))
])

# TREINAMENTO
pipeline.fit(x_train, y_train)

# AVALIAÇÃO

y_pred = pipeline.predict(x_test)

print("Acurácia:", accuracy_score(y_test, y_pred))
print("Relatório de Classificação:\n", classification_report(y_test, y_pred, zero_division=1))
print("Precisão:", precision_score(y_test, y_pred, average='macro'))
print("Recall:", recall_score(y_test, y_pred, average='macro'))
print("F1-Score:", f1_score(y_test, y_pred, average='macro'))

# Matriz de confusão
cm = confusion_matrix(y_test, y_pred)

# ====================
# Visualização de Dados
# ====================
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Previsão")
plt.ylabel("Real")
plt.title("Matriz de Confusão")
plt.show()

y_pred = pipeline.predict(x_test)
y_pred_proba = pipeline.predict_proba(x_test)[:, 1]

novos_comentarios = ['Eu gosto muito de futebol', 'Eu gostaria que as ruas fossem mais seguras para pessoas como eu', 'Aquela pessoa é muito fã de sertanejo', 'Ele é uma das pessoas mais estilosas que eu conheço', "Hoje é um dia de resistência para a minha comunidade"]
novos_comentarios_proc = [preprocess_text(c) for c in novos_comentarios]
previsoes = pipeline.predict(novos_comentarios_proc)
print("Previsões:", previsoes)

#análise de sentimentos
analisador = create_analyzer(task="sentiment", lang="pt")
comentarios = ["Eu não gosto de homens que não são masculinizados", "Ela é uma mulher num corpo de homem", "Ela é a minha melhor amiga, e a mãe de meus filhos adotivos", "Hoje é um dia de resistência para a minha comunidade"]

print("\nAnálise com pysentimiento (sem tradução):")
for texto in comentarios:
    resultado = analisador.predict(texto)
    print(f"'{texto}' → label: {resultado.output}, scores: {resultado.probas}") #Ele está sabendo analisar as frases de forma positiva e negativa.

# Gerar a matriz de confusão
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=pipeline.classes_)

plt.figure(figsize=(5, 4))
disp.plot(cmap='Blues', values_format='d')
plt.title("Matriz de Confusão")
plt.show()
#Matriz bem definida, conseguindo gerar padrões e com poucos erros nas classes

print("Classes únicas:", np.unique(y_test))
print("Quantidade de classes:", len(np.unique(y_test)))

#MapReduce: contar ocorrencias
#sera legal usar contar para ver a quantidade de crimes que ocorreram
#estamos "formatando" os dados, tirando os colchetes do array e as aspas do texto, etc
def mapper(linhas):
  pares = []
  for linha in linhas:
    if pd.notna(linha):
      for denuncias in linha.replace("[","").replace("[","").replace("'","").split(","):
        denuncias = denuncias.strip()
        pares.append((denuncias, 1))
  return pares
#agrupando, estamos agrupando os dados
def shuffle(mapped_data):
  agrupado = {}
  for chave, valor in mapped_data:
    if chave not in agrupado:
      agrupado[chave] = []
    agrupado[chave].append(valor)
  return agrupado

#fazendo as contagens das ocorrencias
def reducer(agrupado):
  return {chave: sum(valores) for chave, valores in agrupado.items()}

mapeado = mapper(df["TipoDenuncia"])
agrupado = shuffle(mapeado)
resultado = reducer(agrupado)

print("Map Reducer: Denuncias mais frequentes: ")
print(resultado)

#HIVE: SQL COM PANDAS
print("HIVE: Denuncias em pessoas abaixo de 18 anos: ")
df_hive = df[["TipoDenuncia", "Idade"]]
df_hive['Idade'] = pd.to_numeric(df_hive['Idade'], errors='coerce')
print(df_hive.query("Idade < 18"))

#HBase: banco NOSQL orientado a colunas
#muito usado para dados nao estruturados que nao sao armazenados em sql

# Passo 1: Converter a coluna 'idade' para numérico, substituindo valores inválidos por NaN

# =======================
# Pré-processamento de Dados
# =======================
df['Idade'] = pd.to_numeric(df['Idade'], errors='coerce')

# Passo 2: Remover as linhas com 'idade' inválida (NaN)
df_clean = df.dropna(subset=['Idade'])

# Passo 3: Criar o dicionário hbase com as linhas limpas
hbase = {
    f"TipoViolencia:{i}": {
        "info:Idade": int(row["Idade"]), "info:PeriodoDia": row["PeriodoDia"], "info:IdentidadeGenero": row["IdentidadeGenero"]
    }
    for i, row in df_clean.iterrows()
}
print("Hbase: Leitura simulada dos dados: ")
#como é uma base de dados colunar, é necessário um for para visualizar os dados
for chave, colunas in hbase.items():
  print(f"{chave} => Idade: {colunas['info:Idade']},   Periodo do Dia: {colunas['info:PeriodoDia']},   Identidade de Gênero: {colunas['info:IdentidadeGenero']}")

#Sqoop: simular HDFS
conn = sqlite3.connect(":memory")
df[["TipoDenuncia", "Idade", "PeriodoDia", "IdentidadeGenero"]].to_sql("denuncias", conn, if_exists="replace", index=False)

#antes da limpeza
df_sql = pd.read_sql("SELECT * FROM Denuncias", conn)
df_sql.to_csv("denuncias_trans_idade_hdfs.csv", index=False)


# =====================
# Carregamento de Dados
# =====================
df_hdfs = pd.read_csv("denuncias_trans_idade_hdfs.csv")
print("Dados HDFS: ")
print(df_hdfs)

#COM LIMPEZA

# Criar uma conexão SQLite em memória
conn = sqlite3.connect(":memory:")

# Supondo que df já exista no seu código, vamos salvar os dados em uma tabela SQL
df[["TipoDenuncia", "Idade", "PeriodoDia", "IdentidadeGenero"]].to_sql("denuncias", conn, if_exists="replace", index=False)

# Carregar os dados da tabela SQL para o DataFrame
df_sql = pd.read_sql("SELECT * FROM denuncias", conn)

# Passo 1: Converter a coluna 'idade' para numérico e substituir valores inválidos (não numéricos) por NaN
df_sql['Idade'] = pd.to_numeric(df_sql['Idade'], errors='coerce')

# Passo 2: Remover as linhas com NaN na coluna 'idade'
df_clean = df_sql.dropna(subset=['Idade'])

# Passo 3: Remover as linhas onde 'tipo_de_denuncia' é a string 'Invalido'
df_clean = df_clean[df_clean['TipoDenuncia'] != 'Invalido']

# Passo 4: Garantir que 'idade' seja um número inteiro (se necessário)
df_clean['Idade'] = df_clean['Idade'].astype(int)

# Passo 5: (Opcional) Remover valores NaN em outras colunas, se necessário
df_clean = df_clean[df_clean['PeriodoDia'].notna()]
df_clean = df_clean[df_clean['IdentidadeGenero'].notna()]

# Agora, você pode exportar os dados limpos para um arquivo CSV
df_clean.to_csv("denuncias_trans_idade_hdfs.csv", index=False)

# Ler o CSV e simular o HDFS
df_hdfs = pd.read_csv("denuncias_trans_idade_hdfs.csv")

# Exibir os dados limpos
print("Dados HDFS: ")
print(df_hdfs)

print("--- Simulação de Data Lake ---")

# Criar um diretório para o Data Lake, se não existir
data_lake_dir = "data_lake_raw"
os.makedirs(data_lake_dir, exist_ok=True)

# Exemplo de dados brutos (simulando um log ou evento em JSON)
raw_data_entry1 = {
    "timestamp": "2023-01-15T10:30:00Z",
    "event_type": "denuncia_recebida",
    "payload": {
        "id_denuncia": "DL001",
        "tipo_bruto": "violencia_fisica_grave",
        "detalhes_texto": "Agressão em via pública, suspeito evadiu. Idade aparente: 25. Periodo: Noite.",
        "localizacao_geo": "lat:-23.5,lon:-46.6"
    },
    "source": "app_mobile"
}

raw_data_entry2 = {
    "timestamp": "2023-01-15T11:00:00Z",
    "event_type": "denuncia_recebida",
    "payload": {
        "id_denuncia": "DL002",
        "tipo_bruto": "discriminacao_verbal",
        "detalhes_texto": "Comentarios ofensivos sobre identidade de genero. Idade aparente: 19. Periodo: Tarde.",
        "localizacao_geo": "lat:-23.6,lon:-46.7"
    },
    "source": "web_form"
}

# Salvar os dados brutos no Data Lake como arquivos JSON
file_path1 = os.path.join(data_lake_dir, "raw_denuncia_001.json")
with open(file_path1, 'w', encoding='utf-8') as f:
    json.dump(raw_data_entry1, f, ensure_ascii=False, indent=4)
print(f"Dados brutos salvos em: {file_path1}")

file_path2 = os.path.join(data_lake_dir, "raw_denuncia_002.json")
with open(file_path2, 'w', encoding='utf-8') as f:
    json.dump(raw_data_entry2, f, ensure_ascii=False, indent=4)
print(f"Dados brutos salvos em: {file_path2}")

# Exemplo de como ler dados do Data Lake
print("\nLendo dados do Data Lake:")
loaded_data = []
for filename in os.listdir(data_lake_dir):
    if filename.endswith(".json"):
        filepath = os.path.join(data_lake_dir, filename)
        with open(filepath, 'r', encoding='utf-8') as f:
            loaded_data.append(json.load(f))

for data_entry in loaded_data:
    print(f"  Evento: {data_entry['event_type']}, ID: {data_entry['payload']['id_denuncia']}")


# --- Simulação de um Data Warehouse ---
# Um Data Warehouse armazena dados estruturados, limpos e transformados,
# otimizados para consultas analíticas e relatórios.
# Vamos simular um processo ETL (Extract, Transform, Load) para popular o DW.

# --- Simulação de um Data Warehouse ---
# Um Data Warehouse armazena dados estruturados, limpos e transformados,
# otimizados para consultas analíticas e relatórios.
# Vamos simular um processo ETL (Extract, Transform, Load) para popular o DW.

print("\n--- Simulação de Data Warehouse (Processo ETL) ---")

# 1. Extract (Extrair): Ler dados do Data Lake (ou do seu DataFrame 'df' já limpo)
# Para este exemplo, vamos usar o seu DataFrame 'df' já carregado e pré-processado,
# pois ele já representa uma fonte de dados mais estruturada para o DW.
# Se estivéssemos usando os dados brutos do Data Lake, haveria mais etapas de parsing.

# Certifique-se de que 'df' está disponível (assumindo que as células anteriores foram executadas)
if 'df' not in locals():
    print("DataFrame 'df' não encontrado. Por favor, execute as células anteriores para carregar 'df'.")
    # Exemplo de carregamento de df se não estiver disponível
    # from google.colab import drive
    # drive.mount('/content/drive')
    # df = pd.read_csv('/content/drive/MyDrive/dados_tratados2.csv', sep = ";")
    # print("DataFrame 'df' carregado para a simulação do Data Warehouse.")

# --- Função de Tratamento de Valores Inválidos ---
def clean_dataframe(df_input):
    """
    Trata valores inválidos em colunas específicas de um DataFrame para o dataset dados_tratados2.csv.

    Parâmetros:
    df_input (pd.DataFrame): O DataFrame a ser limpo.

    Retorna:
    pd.DataFrame: O DataFrame limpo.
    """
    df_cleaned = df_input.copy()

    # --- Tratamento da coluna 'Idade' ---
    # Converte 'Idade' para numérico, transformando erros em NaN
    if 'Idade' in df_cleaned.columns:
        df_cleaned['Idade'] = pd.to_numeric(df_cleaned['Idade'], errors='coerce')
        # Remove linhas onde 'Idade' é NaN
        df_cleaned.dropna(subset=['Idade'], inplace=True)
        # Converte para inteiro após a limpeza (se necessário e se não houver float)
        df_cleaned['Idade'] = df_cleaned['Idade'].astype(int)

    # --- Tratamento de valores textuais "Invalido" ---
    # Remove linhas onde 'TipoDenuncia' ou outras colunas categóricas contêm a string 'Invalido'
    columns_to_check_for_invalido = ['TipoDenuncia', 'TipoViolencia', 'IdentidadeGenero', 'OrientacaoSexual', 'Etnia', 'Escolaridade', 'PeriodoDia', 'GeneroBiologico']
    for col in columns_to_check_for_invalido:
        if col in df_cleaned.columns:
            # Garante que a coluna é do tipo string antes de aplicar o filtro de string
            if df_cleaned[col].dtype == 'object':
                df_cleaned = df_cleaned[df_cleaned[col] != 'Invalido']
            # Opcional: remover strings vazias ou espaços em branco
            if df_cleaned[col].dtype == 'object':
                 df_cleaned = df_cleaned[df_cleaned[col].astype(str).str.strip() != '']


    # --- Tratamento de NaNs em outras colunas críticas ---
    # Remove linhas com NaN em colunas categóricas que são usadas em análises ou gráficos
    columns_to_dropna_categorical = ['PeriodoDia', 'IdentidadeGenero', 'TipoViolencia', 'OrientacaoSexual', 'Etnia', 'Escolaridade', 'GeneroBiologico']
    for col in columns_to_dropna_categorical:
        if col in df_cleaned.columns:
            df_cleaned.dropna(subset=[col], inplace=True)

    return df_cleaned

# 2. Transform (Transformar): Limpar, agregar e estruturar os dados para o DW
# Vamos focar em algumas agregações úteis para análise.

# Exemplo de transformação: Contagem de denúncias por TipoDenuncia e PeriodoDia
df_dw_aggregated = df.groupby(['TipoDenuncia', 'PeriodoDia']).size().reset_index(name='Contagem')
print("\nDados agregados para o Data Warehouse (Contagem por TipoDenuncia e PeriodoDia):")
print(df_dw_aggregated.head())

# Exemplo de transformação: Média de Idade por Identidade de Gênero
df_dw_avg_idade_genero = df.groupby('IdentidadeGenero')['Idade'].mean().reset_index()
print("\nDados agregados para o Data Warehouse (Média de Idade por IdentidadeGenero):")
print(df_dw_avg_idade_genero.head())

# 3. Load (Carregar): Armazenar os dados transformados em um formato de DW (e.g., outro DataFrame, arquivo parquet/csv)
# Para simulação, vamos manter em DataFrames Pandas, mas em um DW real, seria um banco de dados.
data_warehouse_tables = {
    "denuncias_agregadas_periodo": df_dw_aggregated,
    "media_idade_genero": df_dw_avg_idade_genero
}
print("\nDados carregados nas 'tabelas' do Data Warehouse (DataFrames Pandas).")

"""***GRAFICO INTERATIVO***"""

# Instalações essenciais (só uma vez por ambiente)
#!pip install -q dash dash-bootstrap-components jupyter-dash

# Ativa renderização no Colab
import plotly.io as pio
pio.renderers.default = 'colab'

# Evita duplicacoes no Colab
if 'app' in globals():
    del app

# Imports principais
from dash import dcc, html, Input, Output
import dash_bootstrap_components as dbc
import plotly.express as px
import pandas as pd
import requests

# Dicionário de siglas
sigla_para_nome = {
    'AC': 'Acre', 'AL': 'Alagoas', 'AP': 'Amapá', 'AM': 'Amazonas', 'BA': 'Bahia',
    'CE': 'Ceará', 'DF': 'Distrito Federal', 'ES': 'Espírito Santo', 'GO': 'Goiás',
    'MA': 'Maranhão', 'MT': 'Mato Grosso', 'MS': 'Mato Grosso do Sul', 'MG': 'Minas Gerais',
    'PA': 'Pará', 'PB': 'Paraíba', 'PR': 'Paraná', 'PE': 'Pernambuco', 'PI': 'Piauí',
    'RJ': 'Rio de Janeiro', 'RN': 'Rio Grande do Norte', 'RS': 'Rio Grande do Sul',
    'RO': 'Rondônia', 'RR': 'Roraima', 'SC': 'Santa Catarina', 'SP': 'São Paulo',
    'SE': 'Sergipe', 'TO': 'Tocantins'
}

# Prepara o DataFrame df (deve estar carregado no ambiente)

# =======================
# Pré-processamento de Dados
# =======================
df['Estado'] = df['Estado'].str.upper().str.strip()
df['EstadoNome'] = df['Estado'].map(sigla_para_nome)

# GeoJSON dos estados
geojson_estados = requests.get(
    'https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson'
).json()

# Denúncias por estado
df_estado = df['EstadoNome'].value_counts().reset_index()
df_estado.columns = ['EstadoNome', 'Denuncias']

# Faixa etária
faixa_etaria = lambda idade: "<18" if idade < 18 else "18-29" if idade < 30 else "30-44" if idade < 45 else "45-59" if idade < 60 else "60+"
df['FaixaEtaria'] = df['Idade'].apply(faixa_etaria)

# Colunas para seleção interativa
colunas_invalidas = ['ID', 'IDViolencia', 'Endereco', 'Nome', 'Escolaridade']
colunas_validas = [col for col in df.columns if col not in colunas_invalidas and df[col].nunique() < 50 and df[col].notna().all() and col != 'Idade']
tipos_grafico = ['Barras', 'Pizza', 'Linha']

# Inicializa o app

# ===============
# App com Dash
# ===============
app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])

app.layout = dbc.Container([
    dbc.Row([dbc.Col(html.H2("Painel Interativo de Denúncias", className="text-center mb-4"))]),

    dbc.Row([
        dbc.Col([dcc.Graph(id='mapa')], width=6),
        dbc.Col([
            dcc.Graph(id='grafico_denuncias'),
            html.Div(id='saida')
        ], width=6)
    ]),

    html.Hr(),

    dbc.Row([
        dbc.Col([
            html.Label("Eixo X"),
            dcc.Dropdown(id='dropdown-x', options=[{'label': c, 'value': c} for c in colunas_validas], value=colunas_validas[0])
        ], width=4),
        dbc.Col([
            html.Label("Agrupamento (Y)"),
            dcc.Dropdown(id='dropdown-y', options=[{'label': c, 'value': c} for c in colunas_validas], value=colunas_validas[1])
        ], width=4),
        dbc.Col([
            html.Label("Tipo de Gráfico"),
            dcc.Dropdown(id='dropdown-tipo', options=[{'label': t, 'value': t} for t in tipos_grafico], value='Barras')
        ], width=4),
    ], className='mb-3'),

    dbc.Row([
        dbc.Col(dcc.Graph(id='grafico-interativo'), width=8),
        dbc.Col([

# ====================
# Visualização de Dados
# ====================
            dcc.Graph(figure=px.pie(df, names='GeneroBiologico', title='Por Gênero')),
            dcc.Graph(figure=px.pie(df, names='FaixaEtaria', title='Por Faixa Etária'))
        ], width=4)
    ])
], fluid=True)


# ===============
# App com Dash
# ===============
@app.callback(
    Output('mapa', 'figure'),
    Input('mapa', 'id')
)
def renderiza_mapa(_):
    fig = px.choropleth(
        df_estado, geojson=geojson_estados, locations='EstadoNome',
        featureidkey='properties.name', color='Denuncias',
        color_continuous_scale='Reds', scope='south america',
        title='Denúncias por Estado'
    )
    fig.update_geos(fitbounds="locations", visible=False)
    fig.update_layout(margin=dict(l=0, r=0, t=40, b=0))
    return fig

@app.callback(
    Output('grafico_denuncias', 'figure'),
    Output('saida', 'children'),
    Input('mapa', 'clickData')
)
def mostrar_info_estado(clickData):
    if not clickData:
        return {}, "Clique em um estado para ver os detalhes."

    estado_nome = clickData['points'][0]['location']
    estado_sigla = [k for k, v in sigla_para_nome.items() if v == estado_nome][0]

# =======================
# Pré-processamento de Dados
# =======================
    df_filtrado = df[df['Estado'] == estado_sigla]
    if df_filtrado.empty:
        return {}, f"Nenhuma denúncia registrada para {estado_nome}."

    df_tipo = df_filtrado['TipoDenuncia'].value_counts().head(5).reset_index()
    df_tipo.columns = ['TipoDenuncia', 'Contagem']


# ====================
# Visualização de Dados
# ====================
    fig = px.bar(df_tipo, x='TipoDenuncia', y='Contagem', color='TipoDenuncia', title=f'Top 5 em {estado_nome}', text_auto=True)
    fig.update_layout(showlegend=False)
    return fig, f"Estado selecionado: {estado_nome} ({estado_sigla})"


# ===============
# App com Dash
# ===============
@app.callback(
    Output('grafico-interativo', 'figure'),
    Input('dropdown-x', 'value'),
    Input('dropdown-y', 'value'),
    Input('dropdown-tipo', 'value')
)
def atualizar_grafico(x_col, y_col, tipo):
    if not x_col or not y_col or x_col == y_col:
        return px.scatter(title="Escolha colunas diferentes para X e Y")

    if tipo == 'Pizza':
        df_pizza = df[x_col].value_counts().reset_index()
        df_pizza.columns = [x_col, 'Contagem']

# ====================
# Visualização de Dados
# ====================
        fig = px.pie(df_pizza, names=x_col, values='Contagem', title=f"Distribuição por {x_col}")
    else:
        df_agg = df.groupby([x_col, y_col]).size().reset_index(name='Contagem')
        if tipo == 'Barras':
            fig = px.bar(df_agg, x=x_col, y='Contagem', color=y_col, barmode='group', title=f"{y_col} por {x_col}")
        elif tipo == 'Linha':
            fig = px.line(df_agg, x=x_col, y='Contagem', color=y_col, markers=True, title=f"{y_col} por {x_col}")
        else:
            fig = px.scatter(title="Tipo de gráfico inválido")

    fig.update_layout(height=600)
    return fig

# Para rodar no Colab
app.run_server(mode='inline', port=8050, debug=False)